%!TEX program = xelatex
\documentclass[lang=en,11pt,a4paper,citestyle =authoryear]{elegantpaper}

% 标题
\title{Homework05 - MATH 734}
\author{Boren(Wells) Guan}

% 本文档命令
\usepackage{array,url,stix}
\usepackage{subfigure,listings}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}
\newcommand{\code}[1]{\lstinline{#1}}
\newcommand{\prvd}{$\hfill \qedsymbol$}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Hil}{\mathcal{H}}
\newcommand{\range}{\mathcal{R}}
\newcommand{\nul}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Pb}{\mathbb{P}}
% 文档区
\begin{document}

% 标题
\maketitle

\subsection*{Notation}
Here I use $X \wedge Y$ for $\min(X,Y)$ and $X\vee Y$ for $\max(X,Y)$. r.v. for random variable.

\subsection*{Before Reading:}\par
To make the proof more readable, I will miss or gap some natural or not important facts or notations during my writing. If you feel it hard to see, you can refer the appendix after the proof, where I will try to explain some simple conclusions (will be marked) more clearly. In case that you misunderstand the mark, I will add the mark just after those formulas between \$ and before those between \$\$.\par
And I have to claim that the appendix is of course a part of my assignment, so the reference of it is required. Enjoy your grading!

\subsection*{Ex.1} 
Let $(\Omega,\F,P)$ be a probability space and let $(\mathscr{S}, \mathscr{G})$ be a countable measurable space. Let $(X_n)_{n\geq 0}$ be a sequence of $\mathscr{S}$-valued random variables. Let $\F_n:=\sigma(X_0,\cdots,X_n)$. Show that
\[\F_n = \{\text{the countable union of }\bigcup_{(b_1,\cdots,b_n) \in B_1\times\cdots\times B_n} B_n \{X_1 = b_1,\cdots,X_n = b_n\}|B_1,B_2,\cdots,B_n \in \mathscr{G}\}\]
\vspace{0.5em}\\
\textbf{Sol.} \par
    Denote the set on the RHS to be $\F$. We know $B$ is countable for any $B\in\mathscr{G}$, so $B_1\times B_2\cdots \times B_n$ is countable for any $B_1,B_2,\cdots,B_n \in \mathscr{G}$, notice
    \[
    \bigcup_{(b_1,\cdots,b_n) \in B_1\times\cdots\times B_n} B_n \{X_1 = b_1,\cdots,X_n = b_n\} = \bigcap_{i=1}^n X_i^{-1}(B_i)
    \]
    so $\F \subset \F_n$ and it suffices to show that $\F$ is a $\sigma$-algebra, since
    \[
    \Big(\bigcup_{(b_1,\cdots,b_n) \in B_1\times\cdots\times B_n} B_n \{X_1 = b_1,\cdots,X_n = b_n\}\Big)^c = \Big(\bigcap_{i=1}^n X_i^{-1}(B_i)\Big)^c = \bigcup_{i=1}^n X_i^{-1}(B_i^c)\bigcap_{j\neq i, 1\leq j\leq n}X_j^{-1}(B_j)
    \]
    and hence $\F$ is closed under the complements. Obviously, $\F$ is closded under the countable union and $\Omega,\emptyset\in \F$, so $\F$ is a $\sigma$-algebra and we are done.
\prvd
\vspace{0.5em}

\subsection*{Ex.2} 
Let $(X_t)_{t\geq 0}$ be a Markov chain on a countable sate space $\mathscr{S}$ with transition martrix $P$. Let $r_t$ denote the row vector of the distribution of $X_t$.\par
a. Show that for each $t\geq 0$
\[r_{t+1} = r_tP\]\par
b. Show by induction that for each $t\geq 0$
\[r_t = r_0 P^t\]
\vspace{0.5em}\\
\textbf{Sol.}\par
a. We know
\[
\Pb(X_{t+1} = i) = E(\Pb(X_{t+1} = i|X_t)) = E(P(X_t,i)) = \sum\limits_{j\in\mathscr{S}} P(j,i)\Pb(X_{t} = j)
\]
for any $i\in\mathscr{S}$, which measn $r_{t+1} = r_t P$.\par
b. Since $r_0 = r_0 P^0$ and 
\[
r_t = r_0P^t \implies r_{t+1} = r_tP = r_0P^t
\]
so by the induction we know $r_t = r_0 P^t$.
\vspace{0.5em}

\subsection*{Ex.3}
Let $(X_t)_{t\geq 0}$ be a Markov chain on a countable state space $\mathscr{S}$ with transition matrix $P$.\par
a. Show that for each $x,y \in \mathscr{S}$ and $a,b \geq 1$
\[\Pb(X_{a+b} = y|X_a = x) = P^b(x,y)\]\par
b. Show that for each $x,y \in \mathscr{S}$ and $n,m \geq 1$, we have
\[P^{n+m}(x,y) = \sum\limits_{z\in\mathscr{S}} P^n(x,z)P^m(z,y)\]
\vspace{0.5em}\\
\textbf{Sol.} \par
a. We know
\[
\Pb(X_{a+b} = y, X_a = x) = E(E(\chi_{X_{a+b} = y}\chi_{X_a = x}|X_a)) = E(\chi_{X_a = x}\Pb(X_{a+b}=y|X_a))
\]
and notice
\[
P(X_{a+b-k},y) = \sum\limits_{j \in \mathscr{S}} P(j,y)\chi_{X_{a+b-k} = j}
\]
so we have
\[
E(P(X_{a+b-1},y)|\F_{a+b-2}) = \sum\limits_{j \in \mathscr{S}} P(j,y)E(\chi_{X_{a+b-k} = j}|\F_{a+b-2}) = P^2(X_{a+b-2},y)
\]
and by the induction we know
\[
\Pb(X_{a+b}=y|X_a) = E(\cdots E(\chi_{X_{a+b} = y}|\F_{a+b-1})|\cdots|X_a) = E(P^{b-1}(X_{a+1},y)|\F_a) = P^b(X_a, y)
\]
by MCT of martingale. Therefore we have
\[
\Pb(X_{a+b} = y, X_a = x) = E(\chi_{X_a = x}\Pb(X_{a+b}=y|X_a)) = P^b(x,y)\Pb(X_a = x)
\]
and we have
\[\Pb(X_{a+b} = y|X_a = x) = P^b(x,y)\]
if $\Pb(X_a = x) \neq 0$.\par
b. Notice
\[P^{n+1}(x,y) = \sum\limits_{z\in\mathscr{S}} P^{n-1}(x,z)P^(z,y)\] 
since $P^n = P^{n-1}P$ and then we may use the induction to $m$, assume
\[
P^{n+m-1}(x,y) = \sum\limits_{z\in\mathscr{S}} P^n(x,z)P^{m-1}(z,y)
\]
then we have
\[
P^{n+m}(x,y) = \sum\limits_{z\in\mathscr{S}} P^{n+m-1}(x,z)P(z,y) = \sum\limits_{z\in\mathscr{S}} \sum\limits_{j\in\mathscr{S}} P^n(x,j)P^{m-1}(j,z)P(z,y) = \sum\limits_{j\in\mathscr{j}} P^n(x,j)P^m(j,y)
\]
and we are done since $P^k(x,y) \geq 0$ for any $k,x,y$.
\prvd
\vspace{0.5em}

\subsection*{Ex.4} 
Let $(X_t)_{t\geq 0}$ be a Markov chain on a countable state space $\mathscr{S}$ with transition matrix $P$. Let $f:\mathscr{S} \to \R$ be a function. Let $f= [f(1),f(2),\cdots]^T$ be the column vector representing the reward function $f$. Show that
\[[Pf](i) = \sum\limits_{j\in\mathscr{S}}P(i,j)f(j) = E(f(X_1)|X_0 = i)\]
That is $Pf$ is a function on the state space whose value on each state $i$ is the one-step conditional expectation of $f(X_1)$ given $X_0 = i$.
\vspace{0.5em}\\
\textbf{Sol.} \par
We know
\[
E(f(X_1)|X_0 = i) = \sum\limits_{j\in\mathscr{S}} f(j)\Pb(X_1 = j|X_0 = i) = \sum\limits_{j\in\mathscr{S}}P(i,j)f(j)
\]
and we are done.
\prvd
\vspace{0.5em}

\subsection*{Ex.5} 
Let $(X_t)_{t\geq 0}$ be a Markov chain on a countable state space $\mathscr{S}$ with transition matrix $P$. Let $f:\mathscr{S} \to \R$ be a function. Suppose that if the chain $X_t$ has state $x$ at time $t$, then we get a reward of $f(x)$. Let $r_t$ be the distribution of $X_t$,$v$ be the column vector representing the reward function $f$.\par
a. Show that the expected reward at time $t$ is given by
\[Ef(X_t) = \sum\limits_{i\in\mathscr{S}}f(i)\Pb(X_t = i) = r_tv\]\par
b. Use (a) and Ex.2. to show that
\[Ef(X_t) = r_0P^tv\]\par
c. The total reward up to time $r$ is a r.v. given by $R_t = \sum\limits_{k=0}^t f(X_k)$. Show that
\[ER_t = r_0(I+P+P^2+\cdots+P^t)v\]
\vspace{0.5em}\\
\textbf{Sol.} \par
a. We know
\[
Ef(X_t) = \sum\limits_{i\in\mathscr{S}} f(i)\Pb(X_t = i)
\]
and the RHS equals $r_t v$ exactly.\par
b. By Ex.2. we know $r_t = r_0P^t$ and hence
\[Ef(X_t) = r_0P^tv\]\par
c.We know
\[
ER_t = E\sum\limits_{k=0}^tf(X_k) = \sum\limits_{k=0}^t Ef(X_k) = \sum\limits_{k=0}^t r_0P^kv = r_0(I+P+P^2\cdots+P^t)v
\]
and we are done.
\vspace{0.5em}

\subsection*{Ex.6} 
Let $(X_n)_{n\geq 0}$ be a Markov chain on a countable state space $\mathscr{S}$ with transition martrix $P$. Let $\tau$ denote an a.s. finite stopping time w.r.t. the natural filtration $\F_n$. Show that $(X_{\tau + k})_{k\geq 0}$ is a Markov chain with transition matrix $P$. Furthermore, show that it is independent from $(X_n)_{1\leq n <\tau}$ given $X_{\tau}$.
\vspace{0.5em}\\
\textbf{Sol.} \par
Notice $\tau+n$ is a stopping time w.r.t. $\F_k, k\geq 0$ and then we know
\[
\Pb(\chi_{\tau+n+1} = x|\F_{\tau+n}) = \Pb(\chi_{\tau+n+1} = x|X_{\tau+n})
\]
by the strong Markov property and then notice for any $y \in \mathscr{S}$
\[
\begin{aligned}
E(\Pb(X_{\tau+n+1} = x|X_{\tau+n}); X_{\tau+n} = y) &= \sum\limits_{k\geq 0}\Pb(X_{k+n+1} = x,X_{k+n} = y,\tau = k) \\ &= P(y,x)\sum\limits_{k\geq 0}\Pb(X_{k+n} = y, \tau = k) \\
&= P(y,x)\Pb(X_{\tau+n} = y)
\end{aligned}
\]
so we know
\[
\Pb(\chi_{\tau+n+1} = x|X_{\tau+n}) = P(X_{\tau+n},x)
\]
then let $\F'_{n} = \sigma(X_{\tau}, \cdots,X_{\tau+n})$, then we know $\F'_n \subset \F_{\tau+n}$ and hence
\[
\Pb(X_{\tau+n+1} = x|\F_n') = \Pb(\Pb(X_{\tau+n+1} = x|\F_{\tau+n})|\F_n') = \Pb(\Pb(X_{\tau+n+1} = x|X_{\tau+n})|\F_n') = \Pb(X_{\tau+n+1} = x|X_{\tau+n})
\]
the rest part is directly implied by the strong Markov property.
\vspace{0.5em}

\subsection*{Ex.7} 
Let $P=D^{-1}A$ denote the transition matrix of a random walk on a finite graph $(V,E)$ where $D$ denotes the diagonal matrix of degrees and $A$ denotes the adjacency matrix of $G$. Define a  probability distribution $\pi$ on the node set $V = \{1,2,\cdots,m\}$ as
\[
\pi = \dfrac{1}{2|E|}[deg(1),\cdots,deg(m)]
\]
ans show that $P$ is reversible w.r.t. $\pi$. Deduce that $\pi$ is a stationary distribution for $P$.
\vspace{0.5em}\\
\textbf{Sol.} \par
Notice
\[
P(x,y) = \dfrac{1}{deg(x)}\chi_{(x,y) \in E}
\]
and then we know
\[
\pi(x)P(x,y) = \dfrac{1}{2|E|}\chi_{(x,y)\in E} = \dfrac{1}{2|E|}\chi_{(y,x)\in E} = \pi(y)P(y,x)
\]
since $G$ is not directed and hence $P$ is reversible w.r.t. $\pi$. Then
\[
\pi P(x) = \dfrac{1}{2|E|}\sum\limits_{k=1}^m deg(k)P(k,x) = \sum\limits_{k=1}^m\dfrac{1}{2|E|}\chi_{\chi(k,x) \in E} = \dfrac{1}{2|E|}deg(x) = \pi(x)
\]
and
\[
\sum\limits_{k=1}^m \pi(k) = \dfrac{\sum\limits_{k=1}^m deg(k)}{2|E|} = 1
\]
and hence $\pi$ is a stationary distribution for $P$.
\vspace{0.5em}

\subsection*{LP Ex.1.11.} 
Given a direct proof that the stationary distribtuion for an irreducible chain is unique.
\vspace{0.5em}\\
\textbf{Sol.} \par
For $\pi_1,\pi_2$ stationary distribution for the irreducible chain with transition matrix $P$, we know
\[
\sum\limits_{y\in \mathscr{S}} \pi_1(x)\pi_2(y)P(y,x) = \pi_2P(x)\pi_2(x) = \pi_1 (x)\pi_2(x) = \pi_1 P(x)\pi_2(x) = \sum\limits_{y\in \mathscr{S}} \pi_1(y)\pi_2(x)P(y,x) 
\]
and then we know
\[
\sum\limits_{y\in\mathscr{S}} (\pi_1(x)\pi_2(y)-\pi_1(y)\pi_2(x))P(y,x) = 0
\]
then it is easy to check $\pi_1(x) = 0$ implies $\pi_2(x) = 0$. For any $x\in\mathscr{S}$. If there exists $x$ such that $\pi_1(x)\pi_2(y)-\pi_1(y)\pi_2(x) \leq 0$ and $\pi_1(x)+\pi_2(x) > 0$ for any $y\in\mathscr{S}$, we know $\pi_1(x)\pi_2(y) = \pi_1(y)\pi_2(x)$ for any $y\in\mathscr{S}$ and hence $\pi_2(x) = \pi_(x)$ by sum $\pi_2(y)$ and $\pi_1(y)\pi_2(x)/\pi_1(x)$ if $\pi_1(x) \neq 0$ and hence $\pi_1 = \pi_2$, when $\pi_2(x) \neq 0$ it is similar.
Since $\mathscr{S}$ is assumed to be finite and we are done.
\vspace{0.5em}

\subsection*{LP Ex.1.12.} 
Suppose that $P$ is the transition matrix for an irreducible Markov chain. For a subset $A\subset \mathscr{S}$, define $f(x) = E_x(\tau_A)$. Show that
\[
f(x) = 0, x\in A
\]
and
\[
f(x) = 1 + \sum\limits_{y\in\mathscr{S}}P(x,y)f(y), x\notin A
\]
then $f$ is uniquely determined by the equalities above.
\vspace{0.5em}\\
\textbf{Sol.} \par
We know if $x = A$, then $X_0 = A$ and we have
\[
\tau_A = 0
\]
so $f(x) = E_x(\tau_A) = 0$, for $x\notin A$, we know
\[
\begin{aligned}
f(x) &= \sum\limits_{n\geq 0}n\Pb(\tau_A = n|X_0 = x)\\ &= [\sum\limits_{y\in A}P(x,y) + \sum\limits_{n\geq 2}n\sum\limits_{y\in A^c}\Pb(\tau_A = n | X_1 = y, X_0 =x)P(x,y)]\\
&= \sum\limits_{y\in A^c}\sum\limits_{n\geq 2}(n-1)\Pb(\tau_A = | X_1 = y)P(x,y) + \sum\limits_{y\in A^c}\Pb(\tau_A<\infty|X_1 = y)P(x,y) + \sum\limits_{y\in A}P(x,y)\\
&= \sum\limits_{y\in\mathscr{S}}P(x,y)f(y) + 1
\end{aligned}
\]
and we are done.
\vspace{0.5em}

\subsection*{LP Ex.2.3.} 
Consider a random walk on the path $\{0,1,\cdots,n\}$ in which the walks moves left or righ with equal probability except when at $n$ and $0$. When at the end points, it remains at the current location with probability $1/2$ one unit towards the center with probability $1/2$. Compute the expected time of the walk's absorption at state $0$, given that it starts ar state $n$.
\vspace{0.5em}\\
\textbf{Sol.} \par
We may assume $f(i) = E_i(\tau_{0})$ and we know
\[
f(i) = 1 + \dfrac{1}{2}(f(i-1)+f(i+1))
\]
and $f(n) = 1+\dfrac{1}{2}(f(n)+f(n-1))$ with $f(0) = 0$. We know
\[
(f(i+1)-f(i)) - (f(i)-f(i-1)) = -2
\]
for $1\leq i \leq n-1$ and $f(n)-f(n-1) = 2$, so we know
\[
f(n-k)-f(n-k-1) = 2+2k
\]
for $0\leq k \leq n-1$ and hence
\[
f(n) = f(0) + \sum\limits_{k=0}^{n-1}(2+2k) = 2n + n(n-1) = n(n+1)
\]
and we are done.
\vspace{0.5em}

\subsection*{LP Ex.2.10.} 
Let $S_n$ be the simple random walk on $\Z$. Show that
\[
P(\max_{1\leq j\leq n}|S_j| \geq c) \leq 2P(|S_n| \geq c)
\]
\vspace{0.5em}\\
\textbf{Sol.} \par
Let $M$ be the smallest integer greater than $C$. It suffices to show that 
\[
\Pb(|S_n| \geq M, \tau_M \leq n) \geq \Pb(|S_n| < M, \tau_M \leq n)
\]
and we know
\[
\Pb(|S_n| < M, \tau_M \leq n) = \Pb(|S_n|-|S_\tau|<0, \tau_M \leq n)
\]
notice $|S_j|$ is a simple random walk on positive integer and it come to $1$ with probability $1$ at the state $0$. We know $|S_{\tau+k}|$ is a Markov chain with the same transition matrix with $|S_n|$, so we know
\[
\Pb(|S_\tau+k|<|S_{\tau}|||S_{\tau}|) \leq \Pb(|S_{\tau}+k|\geq |S_{\tau}||S_{\tau}|)
\]
for any integer $k$, and hence
\[
\Pb(|S_n| \geq M, \tau_M \leq n) \geq \Pb(|S_n| < M, \tau_M \leq n)
\]
then we are done.
\vspace{0.5em}


\addappheadtotoc

\end{document}
