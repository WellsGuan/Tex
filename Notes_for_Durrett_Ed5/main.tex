
%%%%%%%%%%%%%%%%中文%%%%%%蓝色标题%%%    
\documentclass[lang=en, color=blue, ]{elegantbook}
%%%使用包
\usepackage{amsmath, amssymb, amstext,mathrsfs}

%%%标题
\title{Notes for Durrett Ed5}
%%%作者
\author{Wells Guan}
%%%封面中间色块
\definecolor{customcolor}{RGB}{102,102,255}
\colorlet{coverlinecolor}{customcolor}
%%%封面图

%%%自定义符号区
    %%% 组合数, 在数学环境中使用
\newcommand{\per}[2]{\left(\begin{array}{c} #1 \\ #2 \end{array}\right)}
\newcommand{\proba}[1]{\mathsf{P}(#1)}
%%%文档
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\WN}{\varepsilon}
\newcommand{\pushop}{\mathscr{B}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\B}{\mathcal{B}}
\begin{document}

%%%封面页

%%%正文

%%% Stochastic Processes
\chapter{}
\section{Martingales}
\begin{quotation}
p.s. for a probability space.\par
r.v. for a random variable.\par
\end{quotation}
\begin{definition}
For a p.s. $(\Omega, \F_0,P)$ a $\sigma$-field $\F \subset \F_0$ and a r.v. $X\in\F_0$ with $E|X|<\infty$. We define the $conditional\ expectation$ of $X$ given $\F$, $E(X|\F)$ to be any r.v. $Y$ that has\par
a. $Y\in\F$.\par
b. $\int_A X dP = \int_A YdP$ for all $A\in\F$.
and $Y$ is said to be a $version$ of $E(X|\F)$.
\end{definition}

\begin{lemma}
    If $Y$ satisfies (a),(b) above, then it is integrable.
\end{lemma}
\begin{proof}\par
    We know
    \[
    \begin{aligned}
    \int_{\{Y>0\}} YdP = \int_{\{Y>0\}} XdP < \infty
    \int_{\{Y<0\}} YdP = \int_{\{Y<0\}} XdP <\infty
    \end{aligned}
    \]
    and hence $\int |Y|dP$ finite.\par
\end{proof}
\begin{lemma}
    If $Y'$ also satisfies (a),(b) in Def.1.1., then $Y=Y'$ a.s.
\end{lemma}
\begin{proof}\par
    Assume $E_n = \{Y'-Y > n^{-1}\}, F_n = \{Y-Y' > n^{-1}\}, n\in \N$, then we know
    \[n^{-1}P(E_n) \leq \int_{E_n} (Y-Y')dP = \int_{E_n}Y dP - \int_{E_n} Y' dP = 0\]
    and hence $P(E_n)=0$ for any $n\in\N$, similarly, we know $P(F_n) = 0$ for any $n\in\N$, therefore, $Y=Y'$ a.s.
\end{proof}

\begin{theorem}
    If $X_1 = X_2$ on $B\in \F$ then $E(X_1|\F) = E(X_2|\F)$ a.s. on $B$.
\end{theorem}
\begin{proof}\par
    For any $E\subset B$, we have
    \[0 = \int_{\{E(X_1|\F)-E(X_2|\F)>n^{-1}\}\cap E} (X_1-X_2)dP \geq n^{-1}P(\{E(X_1|\F)-E(X_2|\F)>n^{-1}\}\cap E)\]
    and the rest is similar.\par
\end{proof}

\begin{theorem}
    $E(X|\F)$ exists.
\end{theorem}
\begin{proof}\par
    Define $\nu(E) = \int_E XdP$ for $E\in\F$ and we know $\nu \ll P$ and hence there exists $Y\in\F$ such that
    \[\int_E YdP = \nu(E) = \int_E XdP\]
    for all $E\in\F$ by Radon-Nikodym's Theorem.
\end{proof}

\begin{example}
    a. If $X\in\F$, then $E(X|\F) = X$.\par
    b. If $X$ is independent to $\F$, i.e. $P(\{X\in B\}\cap A) = P(X\in B)P(A)$, then $X$ is independent to $\chi_A$ for any $A\in\F$ and hence $E(X|\F)= EX$.\par
    c. Suppose $\Omega_1,\Omega_2,\cdots$ is a finite or infinite partition of $\Omega$ into disjoint sets, with $P(\Omega_i) > 0, i\geq 1$ and then let $\F = \sigma(\Omega_1,\Omega_2,\cdots)$ and then
    \[E(X|\F) = \dfrac{E(X;\Omega_i)}{P(\Omega_i)}\quad\text{on }\Omega_i\]\par
    d. Suppose $X,Y$ have joint density $f(x,y)$ i.e.,
    \[P((X,Y)\in B) = \int_B f(x,y) dxdy\quad\text{ for }B\in\mathcal{R}^2\]
    then if $E|g(X)|<\infty$, then $E(g(X)|Y) = h(Y)$, where  
    \[h(y) = \int g(x)f(x,y)dx/\int f(x,y)dx\]
    on $\{(x,y), \int f(x,y) dx > 0\}$, and hence a.s.\par
    e. Suppose $X$ and $Y$ are independent, let $\phi$ be a function with $E|\phi(X,Y)|<\infty$ and let $g(x) = E(\phi(x,Y))$, then $E(\phi(X,Y)|X) = g(X)$.
    \end{example}
\begin{proof}\par
c. By the $\pi-\lambda$ theorem, it suffices to show that
\[\int_A XdP = \int_A YdP\]
for any $A\in\{\bigcup_{1\leq i \leq n}\Omega_i\}$ where $Y$ was defined as above.\par
d. Firstly, we recall any simple function $\phi \geq 0$ will cause $\int \phi(x,y)dy$ is measurable since $\int \phi(x,y)dy = \nu(E_y)$ when $\phi = \chi_E$ and then we know for any $g\geq 0$, $\int g(x)f(x,y)dy$ is measurable and then $\int g(x)f(x,y)dy$ is measurable for general $g$, then we will know $h(Y)\in\sigma(Y)$.\par 
Consider $A\in \sigma(Y)$, where $A = \{Y\in B\}$, then
\[E(h(Y);A) = \int_{Y\in B} h(y)f(x,y)dx dy = \int_B \int h(y)f(x,y)dxdy = \int_B \int g(x)f(x,y)dxdy = E(g(X);A)\]
and the conclusion goes.\par
e. We know $g(X)\in \sigma(X)$ and then for any $A = \{X\in B\}$, we will know
\[E(g(X);A) = \int_B g(x)dx = \int_B \int \phi(x,y) dy dx = E(\phi(X,Y);A)\]
and hence $E(\phi(X,Y)|X) = g(X)$.
\end{proof}

\begin{definition}
Denote
\[
\begin{aligned}
P(A|\mathcal{G}) &= E(1_A|\mathcal{G}) \\
P(A|B) &= P(A\cap B)/P(B) \\
\end{aligned}
\]
and $E(X|Y) = E(X|\sigma(Y))$.
\end{definition}

\begin{theorem}
    For the first two parts, we assume $E|X|, E|Y|<\infty$.\par
    (a) $E(aX+Y|\F) = aE(X|\F)+E(Y|\F)$.\par
    (b) If $X\leq Y$ then $E(X|\F) \leq E(Y|\F)$.\par
    (c) If $X_n \geq 0$ and $X_n\uparrow X$ with $EX<\infty$ then $E(X_n|\F)\uparrow E(X|\F)$.
\end{theorem}

\begin{theorem}
    If $\phi$ is convew and $E|X|,E|\phi(X)|< \infty$ then
    \[\phi(E(X|\F)) \leq E(\phi(X)|\F)\]
\end{theorem}
\begin{proof}\par
Let $S=\{(a,b):a,b\in\Q, ax+b \leq \phi(x)\text{ for all }x\}$, then $\phi(x) = \sup\{ax+b:(a,b)\in S\}$. And we know
\[E(\phi(X)|\F)\geq aE(X|\F)+b\]
and hence $E(\phi(X)|\F) \geq \phi(E(X|\F))$.\par
\end{proof}

\begin{theorem}
    $Conditional\ expectation$ is a contraction in $L^p$, $p\geq 1$.\par
\end{theorem}
\begin{proof}\par
By Theorem 1.5., we have $|E|(X|\F)|^p \leq E(|X|^p|\F)$, then we know 
\[E(|E(X|\F)|^p) \leq E(E(|X|^p|\F)) = E|X|^p\]
\end{proof}

\begin{theorem}
    If $\F\subset\mathcal{G}$ and $E(X|\mathcal{G}) \in \F$, then $E(X|\F) = E(X|\mathcal{G})$. 
\end{theorem}

\begin{theorem}
    If $\F_1\subset\F_2$ then\par
    (i) $E(E(X|\F_1)|\F_2) = E(X|\F_1)$\par
    (ii) $E(E(X|\F_2)|\F_1) = E(X|\F_1)$\par
\end{theorem}
\begin{proof}\par
    For $A\in \F_1$, we know
    \[
    \begin{aligned}
    \int_A E(E(X|\F_1)|\F_2) dP &= \int_A E(X|\F_1) dP = \int_A X dP \\ 
    \int_A E(E(X|\F_2)|\F_1) dP &= \int_A E(X|\F_2) dP = \int_A X DP
    \end{aligned}
    \]
    therefore, the equalities go.
\end{proof}

\begin{theorem}
    If $X\in\F$ and $E|Y|, E|XY| < \infty$ then
    \[E(XY|\F) = XE(Y|\F)\]
\end{theorem}
\begin{proof}\par
    For any $X,Y\geq 0$, assume $\phi_n \uparrow X$ simple, then we know $\phi_nY \uparrow XY$ and then
    \[\int_A E(\chi_BY|\F) = \int_A \chi_BY dP = \int_{AB} YdP = \int_{AB} E(Y|\F)dP = \int_A \chi_BE(Y|\F)\]
    for any $A,B\in\F$ and hence $E(\chi_BY|\F) = \chi_BE(Y|\F)$ for any $B\in\F$, therefore, we know $E(\phi_nY|\F) = \phi_nE(Y|\F)$. By theorem 1.3 we know $E(\phi_nT|\F)\uparrow E(XY|\F)$ and hence $E(XY|\F) = XE(Y|\F)$, so for any $X\in\F, E|Y|<\infty, E|XY|<\infty$, we can consider the positive and negative parts and the conclusion goes.
\end{proof}

\begin{theorem}
    Suppose $EX^2 < \infty$. $E(X|\F)$ is the variable $Y\in\F$ that minimizes the "mean square error" $E(X-Y)^2$.
\end{theorem}
\begin{proof}\par
    If $Z\in L^2(\F)$, then
    \[ZE(X|\F) = E(ZX|\F)\]
    then we know
    \[E(ZE(X|\F)) = E(E(ZX|\F)) = E(ZX)\]
    and hence $E(Z(X-E(X|\F))) = 0$ for any $Z\in L^2(\F)$.\par
    If $Z = E(X|\F)-Y$, then
    \[E(X-Y)^2 = E(X-E(X|\F)+Z)^2 = E(X-E(X|\F))^2 + EZ^2\]
    and hence $E(X-Y)^2$ are minimal when $Y = E(X|\F)$.\par
\end{proof}

\begin{definition}
Let $(\Omega, \F, P)$ be a probability space, $X:(\Omega,\F)\to (S,\mathcal{S})$ and $\mathcal{G}$ a $\sigma$-algebra contained by $\F$. $\mu: \Omega\times \mathcal{S} \to [0,1]$ is said to be a $regular\ conditional\ distribution$ for $X$ given $\mathcal{G}$ if\par
a. For each $A$, $\omega \to \mu(\omega, A)$ is a version of $P(X\in A|\mathcal{G})$.\par
b. For a.e. $\omega$, $A\to \mu(\omega, A)$ is a probability measure on $(S,\mathcal{S})$.\par
When $S=\Omega$ and $X$ is the identity map, $\mu$ is called a $regular\ condition\ probability$.
\end{definition}

\begin{proposition}
    Suppose $X$ and $Y$ have a joint density $f(x,y)>0$. If
    \[\mu(y,A) = \int_A f(x,y)dx/\int f(x,y)dx\]
    then $\mu(Y(\omega),A)$ is a r.c.d for $X$ given $\sigma(Y)$.
\end{proposition}
\begin{proof}\par
    Here we know $X:(\Omega.\F) \to (\R,\mathcal{R})$, so we should check:\\
    a. $\mu(Y(\omega), A) = \int_A f(x,Y(\omega))dx/\int f(x,Y(\omega))dx$ is a version of $P(X\in A|Y)$.\\
    b. For a.e. $\omega$, $\mu_{Y(\omega)}(A) = \mu(Y(\omega),A)$ is a probability measure on $(\R,\mathcal{R})$.\par
    To see the first claim, consider
    \[
    \begin{aligned}
    \int_{Y\in B} P(X\in A|Y) dP &= \int_{Y\in B} \chi_{X\in A} dP = \int_B\int_A f(x,y)dxdy \\&= \int_A \int_B f(x,y)dy dx \\
    & = \int_B \int_A f(x,y)dx dy \\
    =\int_B \int \int_A f(x,y) dx / \int f(x,y)dx f(x,y) dx dy 
    &= \int_{Y\in B} \mu(Y(\omega), A) dP
    \end{aligned}
    \]
    and the second claim is trivial.
\end{proof}
\begin{theorem}
    Let $\mu(\omega,A)$ be a r.c.d for $X$ given $\F$. If $f:(S,\mathcal{S}) \to (\R,\mathcal{R})$ has $E|f(X)|<\infty$ then
    \[E(f(x)|\F) = \int \mu(\omega,dx)f(x)\quad a.s.\]
\end{theorem}
\begin{proof}\par
    Consider $f= \chi_A$ for some $A$ mrb in $\mathcal{R}$, then $\int \mu(\omega, dx)f(x) = \mu(\omega, A) = P(X\in A|\F)$
    and hence the equality holds for all simple functions, then the problem goes.\par
\end{proof}

Here we skip some properties of regular conditional distribution and continue to martingale.\par

\begin{definition}
    $\F_n$ is a filtration, i.e. an increasing sequence of $\sigma$-fields. A sequence $X_n$ is said to be adapted to $\F_n$ if $X_n\in\F_n$ for all $n$. If $X_n$ is sequence with\par
    a. $E|X_n| < \infty$.\par
    b. $X_n$ is adapted to $\F_n$.\par
    c. $E(X_{n+1}|\F_n) = X_n$ for all $n$
    then $X$ is said to be a $martingale$ (resp to $\F_n$). If we replace the equality into $\leq$ or $\geq$, then $X$ is said to be a $supermartingale$ or $submartingale$.\par 
\end{definition}

\begin{example}
    (Random walk)Let $\xi_1,\xi_2,\cdots$ be independent and id.d, $S_n = S_0 +\sum\limits_{i=1}^n \xi_i$ where $S_0$ is a constant. $\F_n = \sigma(\xi_1,\cdots,\xi_n)$ and take $\F_0 = \{\emptyset,\Omega\}$.\par
    a. If $\mu = E\xi_i = 0$ then $S_n, n\geq 0$ is a martingale with respect to $\F_n$.\par
    b. $\mu = E\xi_i = 0$ and $\sigma^2 = \text{var}(\xi_i)<\infty$, then $S_n^2-n\sigma^2$ is a martingale.\par
\end{example}
\begin{proof}\par
    a. Notice $E|S_n| < \infty, n\geq 0$, for any $A \in \F_n$, then notice
    \[E(S_{n+1}|\F_n) = E(\xi_{n+1}|\F_n) + S_n = E\xi_{n+1} + S_n = S_n\]\par
    b. Notice that $E|S_n - n\sigma^2|< \infty$, and
    \[E(S_{n+1}^2-(n+1)\sigma^2|\F) = S_n^2-(n+1)\sigma^2 + \sigma^2 = S_n^2 - n\sigma^2 \]
\end{proof}

\begin{example}
Let $Y_1,Y_2,\cdots$ be nonnegative i.i.d r.v.s with $EY_m = 1$. If $\F_n = \sigma(Y_1,\cdots,Y_n)$, then $M_n = \prod_{m\leq n} Y_m$ defines a martingale.\par
Then assume $\phi(\theta) = Ee^{\theta \xi_i},Y_i = e^{\theta \xi_i}/\phi(\theta)$, then we know $M_n = e^{\theta S_n}/ \phi(\theta)^n$.
\end{example}

\begin{theorem}
    If $X_n$ is a (super-/sub-)martingale then for $n>m$, $E(X_n|\F_m)\leq(\geq/=) X_m$.
\end{theorem}
\begin{proof}
    Notice
    \[E(X_{m+k}|\F_m) = E(E(X_{m+k}|\F_{m+k-1})|\F_m) \leq E(X_{m+k-1}|\F_m)\]
    the rest proof is similar.
\end{proof}

\begin{theorem}
    If $X_n$ is a martingale w.r.t. $\F_n$ and $\phi$ is a convex function with $E|\phi(X_n)| < \infty$ for all $n$ then $\phi(X_n)$ is a submartingale w.r.t. $\F_n$. Consequently, if $p\geq 1$ and $E|X_n|^p < \infty$ for all $n$, then $|X_n|^p$ is a submartingale w.r.t. $\F_n$.
\end{theorem}
\begin{proof}
    Notice
    \[E(\phi(X_{n+1})|\F_n) \geq \phi(E(X_{n+1})|\F_n) = \phi(X_n)\]
    and the problem goes.
\end{proof}

\begin{theorem}
    If $X_n$ is a submartingale w.r.t. $\F_n$ and $\phi$ is an increasing convex function with $E|\phi(X_n)| < \infty$ for all $n$, then $\phi(X_n)$ is a submartingale w.r.t. $\F_n$. Consequently\par
    a. If $X_n$ is a submartingale then $(X_n-a)^+$ is a submartingale.\par
    b. If $X_n$ is a supermartingale then $\min(X_n,a)$ is a supermartingale.
\end{theorem}
\begin{proof}
    Notice
    \[E(\phi(X_{n+1})|\F_n) \geq \phi(E(X_{n+1})|\F_n) \geq \phi(X_n)\]
    then (a) is easy to be checked and hence (b) is correct.
\end{proof}

\begin{definition}
    Let $\F_n, n\geq 0$ be a filtration. $H_n, n\geq 1$ is said to be a $predictable\ sequence$ if $H_n\in\F_{n-1}$ for all $n\geq 1$.
\end{definition}

\begin{definition}
    We denote
    \[(H\cdot X)_n = \sum\limits_{m=1}^n H_m(X_m-X_{m-1})\]
\end{definition}

\begin{theorem}
    Let $X_n, n\geq 0$ be a supermartingale. If $H_n\geq 0$ is predictable and each $H_n$ is bounded then $(H\cdot X)_n$ is a supermartingale.
\end{theorem}
\begin{proof}
    Consider
    \[E((H\cdot X)_{n+1}|\F_n) = E(\sum\limits_{m=1}^{n+1} H_m(X_m-X_{m-1})|\F_n) = (H\cdot X)_n + E(X_{n+1}|\F_n) - X_n \leq (H\cdot X)_n\]
\end{proof}

\begin{definition}
    A r.v. $N$ is said to be a $stopping\ time$ if $\{N=n\}$ in $\F_n$ for all $n>\infty$.
\end{definition}
\begin{theorem}
    If $N$ is a stoppiing time and $X_n$ is a supermartingale, then $X_{N\wedge n}$ is a supermartingale.
\end{theorem}
\begin{proof}
    Consider
    \[E(X_{N\wedge n+1}|\F_n) = E(X_{n+1}\chi_{N\geq n+1}+\sum\limits_{k=0}^nX_{k}\chi_{N = k}|\F_n) \leq \chi_{N\geq n+1} X_n + \sum\limits_{k=0}^nX_{k}\chi_{N = k} = X_{N\wedge n}\]
\end{proof}

\begin{definition}
    Suppose $X_n, n\geq 0$ is a submartingale. Let $a<b, N_0 = -1$ and for $k\geq 1$ let
    \[
    \begin{aligned}
        N_{2k-1} &= \inf\{m>N_{2k-2},X_m\leq a\} \\
        N_{2k} &= \inf\{m>N_{2k-1}, X_m \geq b\}
    \end{aligned}
    \] 
    The $N_j$ are stopping times so
    \[
    H_m = \begin{cases}
        1\quad\text{ if }N_{2k-1}<m\leq N_{2k}\text{ for some }k\\
        0\quad\text{ otherwise}
    \end{cases}
    \]
    defines a predictable sequence.
\end{definition}
\begin{proof}\par
    Notice
    \[\{N_{2k-1} = n\} = \bigcup_{0\leq m \leq n-1}\{N_{2k-2} = m\}\cap(\bigcap_{ n-1-m\geq k\geq 0}\{X_{m+k} > a\})\cap\{X_n \leq a\}\]
    and
    \[
    \{N_{2k} = n\} = \bigcup_{0\leq m \leq n-1}\{N_{2k-1} = m\}\cap(\bigcap_{ n-1-m\geq k\geq 0}\{X_{m+k} < b\})\cap\{X_n \geq b\}
    \]
    and hence $N_{2k-1},N_{2k}$ are stopping times by induction.\par
    And notice
    \[
    \{N_{2k-1}<m\leq N_{2k}\text{ for some }k\} = \bigcup_{k\geq 0} \{N_{2k-1} \leq m-1\}\cap\{N_{2k}\geq m\} \in \F_{m-1}
    \]
    and hence $H_m$ is predictable.
\end{proof}

\begin{theorem}
    (Upcoming inequality) If $X_m,m\geq 0$, is a submartingale then
    \[(b-a)EU_n \leq E(X_n-a)^+ - E(X_0-a)^+\]
    where $U_n = \sup\{k, N_{2k} \leq n\}$.
\end{theorem}
\begin{proof}
    Here we assume $Y_m = a+(X_m-a)^+$ and we have
    \[
    (b-a)U_n \leq (H\cdot Y)_n
    \]
    let $K_m = 1-H_m$ and then we know that $(K\cdot X)_n$ is a submartingale and then
    \[E(K\cdot X)_n \geq E(K\cdot X)_0 = 0\]
    so we know
    \[E(H\cdot Y)_n \leq E(Y_n-Y_0) = E(X_n-a)^+ - E(X_0-a)^+\]
    since $Y_n-Y_0 = (H\cdot Y)_n + (K\cdot Y)_n$
\end{proof}

\begin{theorem}
    (Martingale convergence theorem) If $X_n$ is a submartingale with $\sup EX_n^+ < \infty$ then as $n\to\infty$, $X_n$ converges a.s. to a limit $X$ with $E|X|<\infty$.
\end{theorem}
\begin{proof}
    We know $(X-a)^+ \leq X^+ |a|$, then we know
    \[EU_n \leq (|a|+EX_n^+)/(b-a)\]
    so $\sup X_n^+$ will imply than $EU < \infty$ where $U = \lim U_n$ and hence for all rational $a,b$, we know
    \[P(\{\liminf X_n < a < b < \limsup X_n\}) = 0\]
    and hence $\lim X_n$ exists a.s. and $EX^+ \leq \liminf EX_n^+ <\ infty$ and hence $X<\infty$ a.s. and notice
    \[EX_n^- = EX_n^+ - EX_n \leq EX_n^+ - EX_0\]
    and hence $EX^- \leq \liminf EX_n^- \leq \liminf EX_n^+ - EX_0 < \infty$
    therefore $E|X|<\infty$. 
\end{proof}

\begin{theorem}
    If $X_n\geq 0$ is a supermartingale then as $n\to\infty$, $X_n\to X$ a.s. and $EX\leq EX_0$.
\end{theorem}
\begin{proof}
    Let $Y_n = -X_n$ and hence a submartingale with $EY_n^+ = 0$, then we know $X_n \to X$ a.s. and we also have
    \[EX \leq \liminf EX_n^+ \leq EX_0\] 
\end{proof}

\begin{proposition}
    The theorem 1.18. provide a method to show that a.s. convergence does not guarantee convergence in $L^1$.
\end{proposition}
\begin{proof}
Let $S_n$ be a symmetric simple random walk with $S_0 = 1$ and $P(\xi_i=1) = P(\xi_i = -1) = \tfrac{1}{2}$, let $N = \inf\{n: S_n = 0\}$ and $X_n = S_{N\wedge n}$. Then we know $X_n$ nonnegative and $EX_n = EX_0=1$ since $X_n$ is a martingale, then we know $X_n \to X$ where $X$ is some r.v. and hence $X = 0$, because there is no way to converge to others and hence $X_n$ do not converge to $X$ in $L^1$.
\end{proof}

\begin{proposition}
    Convergence in probability do not guarantee convergence a.s.
\end{proposition}
\begin{proof}
    Let $X_0 = 0$ and $P(X_k = 1|X_{k-1} = 0) = P(X_k = -1|X_{k-1} = 0) = \tfrac{1}{2k}, P(X_k = 0|X_{k-1} = 0) = 1-\tfrac{1}{k}$ and $P(X_k = kX_{k-1}|X_{k-1} \neq 0) = \tfrac{1}{k}, P(X_k = 0|X_{k-1} \neq 0) = 1-\tfrac{1}{k}$, then we know $X_k \to 0$ in probability, but $P(X_k = 0, k\geq K)$ and it picks discrete values and hence $X_k$ can not converge to $0$ a.s. 
\end{proof}
\end{document}